Model: "sequential_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_1 (Conv2D)            (None, 14, 14, 64)        1664
_________________________________________________________________
leaky_re_lu_1 (LeakyReLU)    (None, 14, 14, 64)        0
_________________________________________________________________
dropout_1 (Dropout)          (None, 14, 14, 64)        0
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 7, 7, 128)         204928
_________________________________________________________________
leaky_re_lu_2 (LeakyReLU)    (None, 7, 7, 128)         0
_________________________________________________________________
dropout_2 (Dropout)          (None, 7, 7, 128)         0
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 4, 4, 256)         819456
_________________________________________________________________
leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 256)         0
_________________________________________________________________
dropout_3 (Dropout)          (None, 4, 4, 256)         0
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 4, 4, 512)         3277312
_________________________________________________________________
leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 512)         0
_________________________________________________________________
dropout_4 (Dropout)          (None, 4, 4, 512)         0
_________________________________________________________________
flatten_1 (Flatten)          (None, 8192)              0
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 8193
_________________________________________________________________
activation_1 (Activation)    (None, 1)                 0
=================================================================
Total params: 4,311,553
Trainable params: 4,311,553
Non-trainable params: 0
_________________________________________________________________
Model: "sequential_4"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
dense_2 (Dense)              (None, 12544)             1266944
_________________________________________________________________
batch_normalization_1 (Batch (None, 12544)             50176
_________________________________________________________________
activation_2 (Activation)    (None, 12544)             0
_________________________________________________________________
reshape_1 (Reshape)          (None, 7, 7, 256)         0
_________________________________________________________________
dropout_5 (Dropout)          (None, 7, 7, 256)         0
_________________________________________________________________
up_sampling2d_1 (UpSampling2 (None, 14, 14, 256)       0
_________________________________________________________________
conv2d_transpose_1 (Conv2DTr (None, 14, 14, 128)       819328
_________________________________________________________________
batch_normalization_2 (Batch (None, 14, 14, 128)       512
_________________________________________________________________
activation_3 (Activation)    (None, 14, 14, 128)       0
_________________________________________________________________
up_sampling2d_2 (UpSampling2 (None, 28, 28, 128)       0
_________________________________________________________________
conv2d_transpose_2 (Conv2DTr (None, 28, 28, 64)        204864
_________________________________________________________________
batch_normalization_3 (Batch (None, 28, 28, 64)        256
_________________________________________________________________
activation_4 (Activation)    (None, 28, 28, 64)        0
_________________________________________________________________
conv2d_transpose_3 (Conv2DTr (None, 28, 28, 32)        51232
_________________________________________________________________
batch_normalization_4 (Batch (None, 28, 28, 32)        128
_________________________________________________________________
activation_5 (Activation)    (None, 28, 28, 32)        0
_________________________________________________________________
conv2d_transpose_4 (Conv2DTr (None, 28, 28, 1)         801
_________________________________________________________________
activation_6 (Activation)    (None, 28, 28, 1)         0
=================================================================
Total params: 2,394,241
Trainable params: 2,368,705
Non-trainable params: 25,536
_________________________________________________________________
2020-11-05 21:00:15.038384: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-11-05 21:00:18.727508: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-11-05 21:00:27.547984: W tensorflow/stream_executor/gpu/redzone_allocator.cc:312] Internal: Invoking GPU asm compilation is supported on Cuda non-Windows platforms only
Relying on driver to perform ptx compilation. This message will be only logged once.
0: [D loss: 0.695566, acc: 0.414062]  [A loss: 0.892701, acc: 0.000000]
1: [D loss: 0.676729, acc: 0.500000]  [A loss: 0.825299, acc: 0.000000]
2: [D loss: 0.612458, acc: 0.998047]  [A loss: 0.825308, acc: 0.000000]
3: [D loss: 0.491974, acc: 0.658203]  [A loss: 1.062080, acc: 0.000000]
4: [D loss: 0.404500, acc: 1.000000]  [A loss: 0.167660, acc: 1.000000]
5: [D loss: 0.429275, acc: 0.515625]  [A loss: 0.949871, acc: 0.074219]
6: [D loss: 0.234200, acc: 0.998047]  [A loss: 0.047419, acc: 1.000000]
7: [D loss: 0.168773, acc: 1.000000]  [A loss: 0.021987, acc: 1.000000]
8: [D loss: 0.130375, acc: 1.000000]  [A loss: 0.012628, acc: 1.000000]
9: [D loss: 0.109002, acc: 0.998047]  [A loss: 0.008244, acc: 1.000000]
10: [D loss: 0.091544, acc: 0.996094]  [A loss: 0.004919, acc: 1.000000]
11: [D loss: 0.074671, acc: 1.000000]  [A loss: 0.003573, acc: 1.000000]
12: [D loss: 0.065274, acc: 0.996094]  [A loss: 0.002062, acc: 1.000000]
13: [D loss: 0.056526, acc: 0.998047]  [A loss: 0.000703, acc: 1.000000]
14: [D loss: 0.044488, acc: 1.000000]  [A loss: 0.000852, acc: 1.000000]
15: [D loss: 0.034300, acc: 1.000000]  [A loss: 0.002484, acc: 1.000000]
16: [D loss: 0.032712, acc: 0.996094]  [A loss: 0.000099, acc: 1.000000]
17: [D loss: 0.032175, acc: 0.998047]  [A loss: 0.000041, acc: 1.000000]
18: [D loss: 0.025383, acc: 1.000000]  [A loss: 0.000054, acc: 1.000000]
19: [D loss: 0.018852, acc: 1.000000]  [A loss: 0.000055, acc: 1.000000]
20: [D loss: 0.017295, acc: 0.998047]  [A loss: 0.000012, acc: 1.000000]
21: [D loss: 0.013888, acc: 1.000000]  [A loss: 0.000025, acc: 1.000000]
22: [D loss: 0.014706, acc: 0.998047]  [A loss: 0.000014, acc: 1.000000]
23: [D loss: 0.013399, acc: 0.998047]  [A loss: 0.000001, acc: 1.000000]
24: [D loss: 0.010182, acc: 1.000000]  [A loss: 0.000004, acc: 1.000000]
25: [D loss: 0.010988, acc: 0.998047]  [A loss: 0.000000, acc: 1.000000]
26: [D loss: 0.007218, acc: 1.000000]  [A loss: 0.000002, acc: 1.000000]
27: [D loss: 0.004688, acc: 1.000000]  [A loss: 0.000006, acc: 1.000000]
28: [D loss: 0.004976, acc: 1.000000]  [A loss: 0.000000, acc: 1.000000]
29: [D loss: 0.004308, acc: 1.000000]  [A loss: 0.000008, acc: 1.000000]
30: [D loss: 0.004716, acc: 0.998047]  [A loss: 0.000000, acc: 1.000000]
31: [D loss: 0.009135, acc: 0.998047]  [A loss: 0.000000, acc: 1.000000]
32: [D loss: 0.005189, acc: 1.000000]  [A loss: 0.000000, acc: 1.000000]
33: [D loss: 0.002763, acc: 1.000000]  [A loss: 0.000000, acc: 1.000000]
34: [D loss: 0.002296, acc: 1.000000]  [A loss: 0.000001, acc: 1.000000]
35: [D loss: 0.002071, acc: 1.000000]  [A loss: 0.000000, acc: 1.000000]
36: [D loss: 0.001683, acc: 1.000000]  [A loss: 0.000001, acc: 1.000000]
37: [D loss: 0.002024, acc: 1.000000]  [A loss: 0.000000, acc: 1.000000]
38: [D loss: 0.002078, acc: 1.000000]  [A loss: 0.000001, acc: 1.000000]
39: [D loss: 0.002030, acc: 1.000000]  [A loss: 0.000000, acc: 1.000000]
40: [D loss: 0.002790, acc: 1.000000]  [A loss: 0.000031, acc: 1.000000]
41: [D loss: 0.003147, acc: 1.000000]  [A loss: 0.000000, acc: 1.000000]
42: [D loss: 0.005584, acc: 1.000000]  [A loss: 0.284495, acc: 0.894531]
43: [D loss: 1.642079, acc: 0.566406]  [A loss: 18.791180, acc: 0.000000]
44: [D loss: 0.326801, acc: 0.867188]  [A loss: 8.309049, acc: 0.000000]
45: [D loss: 1.488626, acc: 0.617188]  [A loss: 30.975725, acc: 0.000000]
46: [D loss: 4.866025, acc: 0.500000]  [A loss: 0.000506, acc: 1.000000]
47: [D loss: 0.978045, acc: 0.648438]  [A loss: 1.131394, acc: 0.304688]
48: [D loss: 0.564037, acc: 0.716797]  [A loss: 4.188454, acc: 0.000000]
49: [D loss: 0.503636, acc: 0.761719]  [A loss: 4.487575, acc: 0.000000]
50: [D loss: 0.470713, acc: 0.775391]  [A loss: 4.820082, acc: 0.000000]
51: [D loss: 0.420238, acc: 0.822266]  [A loss: 5.232082, acc: 0.000000]
52: [D loss: 0.363813, acc: 0.835938]  [A loss: 5.486753, acc: 0.000000]
53: [D loss: 0.357883, acc: 0.884766]  [A loss: 5.822234, acc: 0.000000]
54: [D loss: 0.257236, acc: 0.943359]  [A loss: 3.758094, acc: 0.000000]
55: [D loss: 0.596384, acc: 0.642578]  [A loss: 11.395876, acc: 0.000000]
56: [D loss: 1.246028, acc: 0.501953]  [A loss: 0.396113, acc: 0.878906]
57: [D loss: 1.095384, acc: 0.505859]  [A loss: 7.454717, acc: 0.000000]
58: [D loss: 0.439461, acc: 0.742188]  [A loss: 2.197922, acc: 0.000000]
59: [D loss: 0.427390, acc: 0.730469]  [A loss: 4.637829, acc: 0.000000]
60: [D loss: 0.138964, acc: 0.996094]  [A loss: 2.826523, acc: 0.000000]
61: [D loss: 0.298292, acc: 0.876953]  [A loss: 4.281187, acc: 0.000000]
62: [D loss: 0.140725, acc: 0.994141]  [A loss: 3.064044, acc: 0.000000]
63: [D loss: 0.281722, acc: 0.888672]  [A loss: 4.443277, acc: 0.000000]
64: [D loss: 0.152520, acc: 0.992188]  [A loss: 3.099127, acc: 0.000000]
65: [D loss: 0.291278, acc: 0.888672]  [A loss: 4.316947, acc: 0.000000]
66: [D loss: 0.174763, acc: 0.992188]  [A loss: 3.018005, acc: 0.000000]
67: [D loss: 0.289489, acc: 0.888672]  [A loss: 3.532654, acc: 0.000000]
68: [D loss: 0.211966, acc: 0.966797]  [A loss: 2.568047, acc: 0.000000]
69: [D loss: 0.216029, acc: 0.947266]  [A loss: 1.909871, acc: 0.031250]
70: [D loss: 0.279869, acc: 0.878906]  [A loss: 1.709277, acc: 0.050781]
71: [D loss: 0.240702, acc: 0.916016]  [A loss: 1.409835, acc: 0.128906]
72: [D loss: 0.313959, acc: 0.849609]  [A loss: 1.744899, acc: 0.054688]
73: [D loss: 0.255686, acc: 0.906250]  [A loss: 1.498974, acc: 0.117188]
74: [D loss: 0.356197, acc: 0.839844]  [A loss: 2.271501, acc: 0.007812]
75: [D loss: 0.244673, acc: 0.916016]  [A loss: 1.256073, acc: 0.152344]
76: [D loss: 0.489745, acc: 0.724609]  [A loss: 3.999763, acc: 0.000000]
77: [D loss: 0.261717, acc: 0.935547]  [A loss: 0.251429, acc: 0.964844]
78: [D loss: 0.598174, acc: 0.666016]  [A loss: 3.225210, acc: 0.000000]
79: [D loss: 0.280678, acc: 0.947266]  [A loss: 0.994067, acc: 0.320312]
80: [D loss: 0.776055, acc: 0.582031]  [A loss: 5.933310, acc: 0.000000]
81: [D loss: 0.720110, acc: 0.560547]  [A loss: 0.174499, acc: 1.000000]
82: [D loss: 0.633818, acc: 0.638672]  [A loss: 1.187390, acc: 0.140625]
83: [D loss: 0.545039, acc: 0.632812]  [A loss: 2.566031, acc: 0.000000]
84: [D loss: 0.361589, acc: 0.916016]  [A loss: 1.446975, acc: 0.035156]
85: [D loss: 0.477043, acc: 0.707031]  [A loss: 2.589941, acc: 0.000000]
86: [D loss: 0.370719, acc: 0.902344]  [A loss: 1.321568, acc: 0.070312]
87: [D loss: 0.585284, acc: 0.605469]  [A loss: 3.514940, acc: 0.000000]
88: [D loss: 0.425342, acc: 0.841797]  [A loss: 0.674141, acc: 0.574219]
89: [D loss: 0.782420, acc: 0.529297]  [A loss: 3.533795, acc: 0.000000]
90: [D loss: 0.494757, acc: 0.753906]  [A loss: 0.547865, acc: 0.734375]
91: [D loss: 0.769054, acc: 0.521484]  [A loss: 2.585416, acc: 0.000000]
92: [D loss: 0.428961, acc: 0.898438]  [A loss: 1.068022, acc: 0.117188]
93: [D loss: 0.577760, acc: 0.603516]  [A loss: 2.418991, acc: 0.000000]
94: [D loss: 0.419349, acc: 0.886719]  [A loss: 1.351694, acc: 0.031250]
95: [D loss: 0.550091, acc: 0.621094]  [A loss: 2.665642, acc: 0.000000]
96: [D loss: 0.415181, acc: 0.902344]  [A loss: 1.234446, acc: 0.062500]
97: [D loss: 0.622339, acc: 0.572266]  [A loss: 3.237727, acc: 0.000000]
98: [D loss: 0.469344, acc: 0.808594]  [A loss: 0.715657, acc: 0.515625]
99: [D loss: 0.788646, acc: 0.511719]  [A loss: 3.113796, acc: 0.000000]
Elapsed: 1.3006645401318868 min

Process finished with exit code 0
